\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\citation{Boll1979}
\citation{Simard1992,Zheng2016}
\citation{MathisMozer1995}
\citation{Witthoftetal2003}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Hopfield1982}
\citation{Hopfield1984}
\citation{Koiran1994}
\citation{Koiran1994}
\citation{Mozer2009,Siegelmann2008}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) energy landscape, (b) attractor net, (c) unfolded attractor net.}}{2}{figure.1}}
\newlabel{fig:energy}{{1}{2}{(a) energy landscape, (b) attractor net, (c) unfolded attractor net}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Noise Suppression Via Attractor Dynamics}{2}{section.2}}
\newlabel{eq:att_dyn}{{1}{2}{Noise Suppression Via Attractor Dynamics}{equation.2.1}{}}
\newlabel{eq:cue}{{2}{2}{Noise Suppression Via Attractor Dynamics}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Percentage noise variance suppression by an attractor net trained on 3 noise levels. In each graph, the number of hidden units and number of attractors to be learned is varied. Evaluation is with noise level 0.250, 50-dimensional inputs, and 10 replications. Error bars indicate $\pm 1$ SEM.}}{3}{figure.2}}
\newlabel{fig:attractor_alone}{{2}{3}{Percentage noise variance suppression by an attractor net trained on 3 noise levels. In each graph, the number of hidden units and number of attractors to be learned is varied. Evaluation is with noise level 0.250, 50-dimensional inputs, and 10 replications. Error bars indicate $\pm 1$ SEM}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}State-Denoised Recurrent Neural Networks}{3}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulations}{3}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parity}{3}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Interpreting model}{4}{subsubsection.4.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Relative learning rate}{4}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Attractor basins}{4}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Forgetting}{4}{subsubsection.4.1.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Attractor dimensionality}{4}{subsubsection.4.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Majority task}{4}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Simple finite-state machine}{4}{subsection.4.3}}
\citation{AndreasKleinLevine2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Larger, Naturalistic Data Sets}{5}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Relationship to Other Literature}{5}{section.5}}
\bibdata{references}
\bibcite{Boll1979}{{1}{1979}{{Boll}}{{}}}
\bibcite{Hopfield1982}{{2}{1982}{{Hopfield}}{{}}}
\bibcite{Hopfield1984}{{3}{1984}{{Hopfield}}{{}}}
\bibcite{Koiran1994}{{4}{1994}{{Koiran}}{{}}}
\bibcite{Mozer2009}{{5}{2009}{{Mozer}}{{}}}
\bibcite{Siegelmann2008}{{6}{2008}{{Siegelmann}}{{}}}
\bibcite{Simard1992}{{7}{1992}{{Simard et~al.}}{{Simard, Victorri, LeCun, and Denker}}}
\bibcite{Witthoftetal2003}{{8}{2003}{{Witthoft et~al.}}{{Witthoft, Winawer, Wu, Frank, Wade, and Boroditsky}}}
\bibcite{Zheng2016}{{9}{2016}{{Zheng et~al.}}{{Zheng, Song, Leung, and Goodfellow}}}
