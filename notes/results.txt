-------------------------------------------------------------------------------
PARITY
-------------------------------------------------------------------------------
arch vanilla seq_len 5 training_epochs 10000

mean accuracy 0.930313
indiv runs  [1.0, 1.0, 0.96875, 1.0, 1.0, 0.53125, 0.90625, 1.0, 1.0, 0.96875, 1.0, 0.96875, 0.90625, 0.96875, 0.96875, 1.0, 0.96875, 0.90625, 0.9375, 1.0, 0.96875, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 0.90625, 1.0, 0.53125, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.65625, 1.0, 0.96875, 0.53125, 0.875, 0.96875, 0.96875, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.53125, 1.0, 0.96875, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.71875, 0.96875, 0.96875, 0.96875, 1.0, 0.90625, 1.0, 0.8125, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 2972.92982456
indiv epochs  [2601, 8801, 3801, 1201, 2001, 2201, 9601, 7201, 1001, 6201, 801, 1401, 1001, 6201, 1401, 1001, 2001, 4401, 1001, 3401, 6201, 601, 601, 801, 5201, 1801, 1001, 5401, 7001, 1001, 4801, 4601, 2001, 2601, 1601, 1001, 3201, 8001, 801, 1001, 2201, 1401, 1001, 1201, 3201, 3801, 4001, 5201, 3601, 3801, 1801, 401, 1801, 1201, 5001, 1001, 2401]


RESULTS WITH ATTRACTOR NET and TANH RNN ("vanilla")

arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 5
lrate prediction 0.008  lrate attractor  0.008
mean accuracy 0.978125
indiv runs  [1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.84375, 0.75, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.84375, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.9375, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.84375, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 1.0]
mean epoch 2079.26086957
indiv epochs  [3601, 8001, 10001, 4001, 601, 801, 2001, 1201, 201, 1201, 1601, 801, 3001, 1001, 401, 1401, 7201, 4801, 2601, 801, 601, 1801, 1401, 3201, 401, 1401, 3401, 801, 601, 1001, 4601, 1001, 2201, 601, 401, 801, 3601, 1401, 4001, 1001, 401, 1401, 601, 9801, 3601, 4801, 1001, 801, 2401, 601, 801, 201, 601, 601, 601, 801, 3201, 5401, 2201, 801, 401, 1201, 401, 8401, 801, 201, 401, 1001, 601]


Attractor dynamics trained based on prediction error

Namespace(arch='tanh', attractor_train_delay=100, display_epoch=200, lrate_attractor=0.0, lrate_prediction=0.008, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, seq_len=5, task='parity', train_attr_weights_on_prediction=True, training_epochs=10000)
********************************************************************
mean train accuracy 0.7575
indiv runs  [1.0, 0.5, 1.0, 0.53125, 1.0, 0.6875, 1.0, 0.71875, 0.625, 0.5, 0.625, 1.0, 0.5, 0.875, 0.5, 1.0, 0.5, 0.5, 0.625, 0.53125, 0.53125, 0.6875, 1.0, 0.53125, 0.5, 0.53125, 0.53125, 1.0, 0.5, 1.0, 1.0, 0.5, 0.5, 0.5, 1.0, 0.5, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 0.53125, 0.71875, 1.0, 0.5, 1.0, 0.6875, 0.53125, 0.53125, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.53125, 0.5, 1.0, 1.0, 0.5, 0.75, 0.78125, 1.0, 0.59375, 0.53125, 0.65625, 0.5, 0.5, 0.75, 0.90625, 0.5, 0.5, 1.0, 0.53125, 0.5, 0.625, 0.5, 0.5625, 0.5, 0.65625, 1.0, 1.0, 0.71875, 0.5, 1.0, 1.0]
mean epoch 1425.3902439
indiv epochs  [2801, 3601, 201, 2001, 201, 201, 8601, 201, 201, 601, 201, 201, 201, 3201, 401, 8001, 201, 401, 401, 201, 401, 5401, 401, 801, 201, 801, 601, 3601, 201, 1201, 601, 601, 1001, 401, 2801, 601, 3401, 1401, 201, 201, 1601]
mean test accuracy 0.0
indiv runs  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]


-------------------------------------------------------------------------------
MAJORITY SIMULATIONS
-------------------------------------------------------------------------------

NO ATTRACTORS
********************************************************************
Namespace(arch='tanh', attractor_train_delay=100, display_epoch=100, lrate_attractor=0.002, lrate_prediction=0.002, n_attractor_steps=0, n_hidden=5, n_replications=100, noise_level=0.25, seq_len=12, task='majority', training_epochs=2500)
********************************************************************
mean train accuracy 0.992813
indiv runs  [1.0, 0.953125, 1.0, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.953125, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.921875, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.921875, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 540.08045977
indiv epochs  [301, 601, 201, 201, 301, 701, 1101, 701, 201, 301, 201, 501, 601, 801, 1101, 501, 301, 301, 601, 701, 401, 701, 801, 801, 201, 2001, 201, 401, 401, 501, 401, 301, 401, 401, 401, 501, 801, 201, 201, 401, 1001, 501, 2201, 1001, 501, 2101, 201, 201, 201, 301, 401, 401, 901, 401, 401, 201, 801, 201, 201, 201, 301, 401, 401, 301, 401, 401, 401, 1301, 601, 501, 301, 301, 301, 401, 601, 201, 601, 301, 501, 1201, 1901, 201, 301, 501, 601, 501, 301]
mean test accuracy 0.864586
indiv runs  [0.93303573, 0.80505955, 0.8732639, 0.89508927, 0.94915676, 0.51041669, 0.86235118, 0.94717264, 0.84052581, 0.94866073, 0.88640875, 0.93998015, 0.89930558, 0.83680558, 0.84722221, 0.83333331, 0.87400794, 0.91145831, 0.91914684, 0.87872022, 0.81349206, 0.81051588, 0.91418654, 0.92807537, 0.88442463, 0.56994045, 0.91220236, 0.91493058, 0.76190478, 0.91716272, 0.89880955, 0.93725199, 0.92162699, 0.9464286, 0.91765875, 0.94345236, 0.90724206, 0.8660714, 0.90897816, 0.87152779, 0.88690478, 0.87127978, 0.91493058, 0.8660714, 0.95882934, 0.81175596, 0.890625, 0.86235118, 0.82390875, 0.87351191, 0.86954367, 0.88144839, 0.94543654, 0.88467264, 0.82266867, 0.91964287, 0.52281743, 0.6857639, 0.91716272, 0.93278772, 0.84027779, 0.52405757, 0.9375, 0.95114088, 0.85863096, 0.53670633, 0.92063493, 0.89831346, 0.96155757, 0.86483133, 0.56870037, 0.87276787, 0.93898809, 0.90079367, 0.89806545, 0.96279764, 0.90401787, 0.89459324, 0.92509919, 0.88095236, 0.86061507, 0.8856647, 0.92485118, 0.92832339, 0.51835316, 0.90699404, 0.92906743, 0.91964287, 0.58134919, 0.93998015, 0.8973214, 0.85019839, 0.8330853, 0.89211309, 0.92509919, 0.87723213, 0.85615081, 0.93055558, 0.91369045, 0.96875]


ATTRACTORS
********************************************************************
Namespace(arch='tanh', attractor_train_delay=100, display_epoch=100, lrate_attractor=0.002, lrate_prediction=0.002, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, seq_len=12, task='majority', training_epochs=2500)
********************************************************************
mean train accuracy 0.994844
indiv runs  [1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.921875, 1.0, 0.859375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0]
mean epoch 697.551724138
indiv epochs  [301, 2001, 601, 301, 701, 401, 501, 501, 601, 301, 1301, 601, 301, 401, 801, 601, 701, 201, 401, 301, 901, 601, 1501, 301, 401, 701, 801, 201, 201, 201, 601, 801, 801, 701, 201, 401, 601, 401, 901, 701, 401, 1901, 401, 801, 301, 301, 201, 801, 801, 701, 701, 401, 801, 701, 601, 201, 401, 601, 601, 601, 201, 501, 2201, 801, 1301, 901, 1101, 1001, 301, 401, 1301, 601, 501, 401, 601, 801, 1601, 1001, 801, 401, 801, 2201, 2201, 201, 1801, 501, 301]
mean test accuracy 0.902617
indiv runs  [0.94221228, 0.94196427, 0.91443455, 0.82465279, 0.91840279, 0.94072419, 0.92931545, 0.87425596, 0.93551588, 0.9754464, 0.94270831, 0.76488096, 0.92881942, 0.91840279, 0.95213294, 0.90153772, 0.88665676, 0.90029764, 0.89136904, 0.8923611, 0.91021824, 0.90376985, 0.97445434, 0.95213294, 0.92038691, 0.93129963, 0.86334324, 0.890625, 0.88368058, 0.93799603, 0.90600199, 0.96974206, 0.89682537, 0.97842264, 0.93675596, 0.89484125, 0.8973214, 0.88343257, 0.93278772, 0.88764882, 0.95610118, 0.93427581, 0.89980161, 0.94990081, 0.91468257, 0.98859125, 0.85119045, 0.88715279, 0.91617066, 0.93353176, 0.85515875, 0.9704861, 0.96453375, 0.87599206, 0.9308036, 0.89781743, 0.59325397, 0.92509919, 0.95014882, 0.51537699, 0.92336309, 0.85515875, 0.96825397, 0.90972221, 0.94866073, 0.78645831, 0.91269839, 0.89533728, 0.91393846, 0.8767361, 0.87475199, 0.6200397, 0.95238096, 0.89583331, 0.90079367, 0.94568455, 0.93998015, 0.97222221, 0.90153772, 0.88740081, 0.79786706, 0.91319442, 0.87872022, 0.90376985, 0.98487103, 0.91617066, 0.97792661, 0.89831346, 0.89459324, 0.97098213, 0.98363096, 0.94990081, 0.5329861, 0.98660713, 0.86160713, 0.90228176, 0.97594243, 0.90525794, 0.95238096, 0.95585316]

ATTRACTOR TRAINED ON PREDICTION TASK

Namespace(arch='tanh', attractor_train_delay=100, display_epoch=100, lrate_attractor=0.0, lrate_prediction=0.002, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, seq_len=12, task='majority', train_attr_weights_on_prediction=True, training_epochs=2500)
********************************************************************
mean train accuracy 0.945781
indiv runs  [1.0, 0.71875, 0.75, 1.0, 1.0, 0.859375, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 0.796875, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.859375, 1.0, 1.0, 0.953125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.84375, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 0.90625, 0.890625, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.75, 0.984375, 1.0, 1.0, 0.984375, 0.984375, 1.0, 0.796875, 0.921875, 0.796875, 1.0, 1.0, 0.984375, 0.984375, 1.0, 1.0, 1.0, 0.859375, 1.0, 1.0, 1.0, 1.0, 0.75, 0.890625, 1.0, 1.0, 1.0, 0.890625, 0.703125, 0.90625, 1.0, 0.9375, 1.0, 0.796875, 1.0, 1.0, 0.765625, 0.734375, 1.0, 0.9375, 0.828125, 1.0, 0.75, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.953125, 1.0, 0.828125]
mean epoch 734.898305085
indiv epochs  [501, 801, 201, 601, 1401, 301, 301, 1801, 301, 1401, 1401, 1001, 601, 401, 301, 101, 301, 401, 1201, 401, 1701, 1101, 401, 501, 201, 201, 401, 1401, 301, 901, 601, 1601, 901, 601, 2001, 401, 301, 1801, 1001, 301, 401, 301, 501, 201, 501, 201, 1701, 401, 501, 201, 2101, 301, 401, 901, 901, 1301, 301, 1701, 201]
mean test accuracy 0.823943
indiv runs  [0.89533728, 0.71403772, 0.61061507, 0.83829367, 0.93874007, 0.56001985, 0.89434522, 0.7705853, 0.8348214, 0.96750993, 0.84375, 0.85615081, 0.62301588, 0.82192463, 0.79637897, 0.90848213, 0.90178573, 0.82390875, 0.85119045, 0.76314485, 0.90749007, 0.93179566, 0.87400794, 0.94717264, 0.9620536, 0.92212301, 0.87202382, 0.92881942, 0.55704367, 0.93625993, 0.78745037, 0.88616073, 0.92311507, 0.88095236, 0.65575397, 0.89186507, 0.9169147, 0.69841272, 0.91344243, 0.87872022, 0.8504464, 0.62772816, 0.88219243, 0.91468257, 0.86036706, 0.81870037, 0.80952382, 0.85317463, 0.63814485, 0.88715279, 0.86259919, 0.8839286, 0.91468257, 0.82787699, 0.85639882, 0.703125, 0.69345236, 0.6736111, 0.8018353, 0.92757934, 0.8526786, 0.85739088, 0.89335316, 0.92633927, 0.89136904, 0.5691964, 0.88343257, 0.9268353, 0.93005955, 0.96230161, 0.62425596, 0.77752978, 0.92584324, 0.90476191, 0.88839287, 0.62177581, 0.49652779, 0.84771824, 0.80332339, 0.70386904, 0.91567463, 0.71750993, 0.91071427, 0.87251985, 0.73115081, 0.5848214, 0.91393846, 0.80431545, 0.60143846, 0.91443455, 0.55828375, 0.85887897, 0.85912699, 0.89682537, 0.91195434, 0.89409721, 0.87425596, 0.87475199, 0.92460316, 0.71527779]



-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
-------------------------------------------------------------------------------
arch GRU SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 0
mean accuracy 0.998438
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1805.12371134
indiv epochs  [2001, 9601, 1401, 2801, 1001, 1601, 1801, 5601, 1401, 1801, 1001, 801, 1201, 1401, 1001, 1201, 2201, 801, 1801, 601, 1801, 1601, 1601, 1601, 801, 1801, 4201, 601, 1001, 1201, 601, 1001, 801, 2001, 1601, 1801, 1401, 3001, 801, 1401, 3201, 2801, 2001, 1001, 1601, 401, 801, 601, 1201, 801, 1401, 601, 1801, 1201, 2601, 601, 2401, 2401, 801, 1001, 1801, 1801, 1401, 1601, 1201, 2001, 7201, 1001, 8601, 7601, 1601, 3601, 2601, 1201, 1001, 1001, 1401, 1201, 1801, 1801, 3601, 1401, 1601, 801, 1201, 1801, 2001, 601, 1201, 1401, 1201, 1401, 601, 1001, 1401, 801, 801]

arch GRU SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3
mean accuracy 0.958438
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1906.61797753
indiv epochs  [1601, 1201, 1201, 1801, 2801, 5801, 1001, 3201, 1201, 1401, 601, 801, 1801, 801, 801, 4801, 401, 1801, 2401, 5401, 801, 2001, 1201, 7601, 401, 1601, 1001, 1201, 2401, 401, 1401, 1601, 1201, 1601, 2201, 3601, 601, 1401, 1801, 1201, 3401, 3201, 801, 401, 601, 601, 801, 601, 7601, 1801, 3801, 401, 3801, 3601, 1401, 6801, 2201, 1801, 1001, 1001, 1201, 2001, 1201, 1601, 801, 1601, 801, 601, 4401, 2001, 1001, 3401, 1001, 2001, 1201, 801, 2601, 1601, 1801, 401, 601, 801, 801, 5001, 401, 3601, 3401, 801, 601]


arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 0
mean accuracy 0.9025
indiv runs  [0.9375, 0.75, 0.75, 1.0, 0.96875, 0.65625, 1.0, 0.96875, 0.96875, 0.71875, 0.96875, 0.96875, 0.9375, 1.0, 0.90625, 1.0, 0.96875, 0.75, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.78125, 0.96875, 0.9375, 0.96875, 1.0, 1.0, 0.53125, 0.9375, 0.625, 1.0, 1.0, 0.96875, 0.8125, 1.0, 0.71875, 1.0, 0.96875, 0.65625, 0.53125, 0.96875, 1.0, 1.0, 1.0, 0.75, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.78125, 0.875, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.53125, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.8125, 0.96875, 0.75, 0.90625, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.5625, 1.0, 0.8125, 0.96875, 1.0, 0.90625, 1.0, 0.96875, 1.0, 0.9375, 1.0, 0.96875]
mean epoch 2986.36585366
indiv epochs  [4601, 2201, 1601, 3801, 2801, 5801, 2401, 3001, 1001, 2001, 2601, 8201, 601, 4001, 6401, 1201, 4001, 5001, 1001, 2001, 1201, 2801, 8801, 2801, 2201, 1201, 3201, 3801, 2601, 801, 1001, 5401, 5601, 2201, 1801, 1401, 3201, 1401, 601, 4001, 2201]

arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3
mean accuracy 0.914062
indiv runs  [0.9375, 1.0, 1.0, 0.75, 1.0, 0.59375, 1.0, 0.8125, 0.9375, 0.75, 0.96875, 0.90625, 0.96875, 1.0, 0.75, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.8125, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 0.65625, 1.0, 1.0, 0.96875, 0.8125, 0.9375, 0.84375, 1.0, 0.96875, 0.53125, 0.75, 0.96875, 0.84375, 1.0, 1.0, 0.75, 1.0, 1.0, 0.8125, 0.53125, 1.0, 0.8125, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.65625, 0.75, 0.6875, 0.96875, 0.9375, 1.0, 1.0, 0.90625, 0.96875, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.96875, 1.0, 0.8125, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.75, 0.8125, 0.96875, 0.96875, 0.84375, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875]
mean epoch 2935.7826087
indiv epochs  [9401, 2401, 2201, 1401, 3801, 3401, 1601, 9401, 2201, 2001, 801, 2601, 3001, 2401, 4401, 1201, 2601, 401, 1001, 5401, 2001, 1001, 601, 8601, 2801, 801, 8601, 1001, 801, 1801, 3001, 1601, 801, 4801, 5201, 9201, 1001, 1401, 1201, 2401, 1001, 7001, 801, 1201, 2601, 2201]


-------------------------------------------------------------------------------
Sat Nov 25 16:14:02 PST 2017
Two noise levels of attractor net training with 5 attractor steps -- beats out 
vanilla in both accuracy and # training epochs for success

arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.947187
indiv runs  [1.0, 1.0, 0.9375, 0.53125, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.875, 0.9375, 0.53125, 0.96875, 1.0, 1.0, 0.84375, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.53125, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 0.96875, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.96875, 0.96875, 1.0, 1.0, 0.96875]
mean epoch 1289.23529412
indiv epochs  [401, 601, 401, 201, 401, 601, 601, 2601, 801, 201, 401, 1001, 1601, 6401, 201, 201, 1201, 201, 401, 401, 401, 401, 3201, 801, 201, 3001, 2201, 201, 3401, 2401, 601, 1401, 201, 2601, 601, 401, 401, 2001, 2401, 401, 401, 601, 3801, 801, 801, 601, 1601, 1001, 201, 1601, 201, 201, 601, 401, 201, 201, 401, 1401, 7201, 2001, 2401, 401, 8801, 601, 601, 201, 601, 3801]

arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.125 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.945625
indiv runs  [1.0, 0.96875, 1.0, 0.53125, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.53125, 0.9375, 0.53125, 0.96875, 1.0, 1.0, 0.84375, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.90625, 0.9375, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 0.96875, 0.90625, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.9375, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1578.77777778
indiv epochs  [801, 1401, 401, 201, 801, 801, 601, 1201, 601, 801, 401, 1401, 1401, 2601, 401, 201, 9201, 4201, 2801, 1201, 801, 1001, 3001, 801, 7201, 401, 4201, 601, 201, 1001, 601, 1601, 2401, 201, 1201, 401, 201, 801, 1601, 3001, 401, 3201, 401, 5401, 2801, 401, 601, 1401, 401, 201, 201, 601, 201, 401, 401, 201, 201, 401, 1201, 9201, 4201, 401, 1601, 2201, 601, 601, 401, 401, 9801, 1001, 801, 801]

update frequency 5
arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.94875
indiv runs  [1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.53125, 0.625, 0.53125, 0.96875, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.53125, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.96875, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1449.64864865
indiv epochs  [401, 801, 3001, 401, 201, 801, 801, 601, 801, 601, 201, 401, 801, 801, 9801, 201, 201, 401, 1201, 401, 2401, 401, 801, 3201, 401, 1801, 601, 201, 801, 2201, 401, 1801, 201, 801, 401, 201, 5201, 2001, 4001, 401, 3801, 401, 2601, 1001, 1801, 601, 401, 1601, 1801, 401, 201, 2401, 2001, 201, 2201, 2001, 201, 801, 401, 401, 401, 401, 1601, 10001, 801, 801, 1601, 601, 3801, 401, 8801, 601, 601, 601]



-------------------------------------------------------------------------------
integrated objective function: denoising + prediction
varying lambda

arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3 
lambda 0.08 mean accuracy 0.91375
lambda 0.04 mean accuracy 0.913125
lambda 0.01 mean accuracy 0.91
lambda 0.02 mean accuracy 0.91375
lambda 0.0  mean accuracy 0.92375
-------------------------------------------------------------------------------

testing attractor net (attractor_test.py)
10 replications, mean test relative distance 

unconstrained + h_out=W*tanh(h_net)+b	0.288803
zero diagonal + h_out=W*tanh(h_net)+b	0.311553
symmetry + h_out=W*tanh(h_net)+b	0.392045
0diag+symm + h_out=W*tanh(h_net)+b	0.430753
unconstrained + h_out=tanh(W*h_net+b)	0.490302
0diag+symm + h_out=tanh(W*h_net+b) 	0.577489
0diag+symm + h_out=tanh(h_net)      	0.979404
  

final xform = linear transform of [-1,+1] scaled output
-------------------------------------------------------------------------------

MIKE: try different variants of attractor net
- lose symmetry and diag zeros
- add linear transform after net
- add bias to internal dynamics
MIKE: allow attractor weights to be trained by prediction objective?
MIKE: specify schedule of updating:  P, A, or B-both
   e.g., PPPPPPPPPPAAAAAAAAAA [and then repeat]


-------------------------------------------------------------------------------
Running Reber

GENERIC RNN
********************************************************************
Namespace(arch='tanh', display_epoch=50, lrate_attractor=0.005, lrate_prediction=0.005, n_attractor_steps=0, n_hidden=10, n_replications=100, noise_level=0.25, seq_len=15, task='reber', train_attr_weights_on_prediction=False, training_epochs=2000)
********************************************************************
mean train accuracy 0.9793
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99000001, 0.62, 1.0, 1.0, 0.97500002, 1.0, 1.0, 1.0, 0.935, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99000001, 1.0, 1.0, 0.97500002, 1.0, 1.0, 1.0, 0.98000002, 0.97500002, 1.0, 0.995, 1.0, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995, 0.99000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.88999999, 0.44999999, 1.0, 1.0, 0.95999998, 0.77499998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95499998, 0.95999998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98000002, 1.0, 1.0, 0.90499997, 1.0, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.82499999, 0.98500001, 1.0, 1.0, 1.0, 0.97500002, 1.0, 1.0, 0.86500001]
mean epoch 402.351351351
indiv epochs  [251, 101, 201, 801, 201, 201, 901, 51, 151, 151, 551, 351, 101, 1201, 351, 101, 151, 151, 301, 101, 1951, 151, 101, 101, 251, 1801, 1051, 1301, 501, 201, 701, 101, 1051, 1201, 501, 151, 151, 251, 201, 101, 1501, 101, 101, 701, 551, 751, 1401, 301, 151, 501, 501, 151, 151, 151, 201, 951, 101, 201, 101, 151, 201, 101, 151, 101, 101, 201, 201, 201, 301, 201, 201, 351, 101, 451]
mean test accuracy 0.83873
indiv runs  [0.93699998, 0.86250001, 0.80400002, 0.85900003, 0.86250001, 0.86549997, 0.83749998, 0.83899999, 0.82499999, 0.63550001, 0.87150002, 0.88749999, 0.79400003, 0.92650002, 0.82999998, 0.82200003, 0.79750001, 0.8355, 0.88300002, 0.81099999, 0.82950002, 0.8955, 0.78250003, 0.847, 0.80549997, 0.84899998, 0.81900001, 0.8775, 0.8495, 0.91649997, 0.85250002, 0.83749998, 0.83050001, 0.86049998, 0.87449998, 0.82999998, 0.7755, 0.85500002, 0.801, 0.84850001, 0.9005, 0.84500003, 0.90149999, 0.85250002, 0.847, 0.83700001, 0.86549997, 0.86000001, 0.92699999, 0.87300003, 0.88450003, 0.7985, 0.83249998, 0.838, 0.86199999, 0.8545, 0.73299998, 0.49900001, 0.90200001, 0.89249998, 0.75599998, 0.62150002, 0.80500001, 0.85799998, 0.82800001, 0.78799999, 0.82999998, 0.81599998, 0.866, 0.83950001, 0.88599998, 0.91500002, 0.80599999, 0.91350001, 0.93900001, 0.88450003, 0.78149998, 0.83850002, 0.84200001, 0.72750002, 0.9285, 0.85500002, 0.80449998, 0.80900002, 0.86750001, 0.88999999, 0.861, 0.82349998, 0.88849998, 0.8355, 0.95450002, 0.72399998, 0.79549998, 0.86000001, 0.84899998, 0.89899999, 0.764, 0.88099998, 0.875, 0.73799998]

********************************************************************
Namespace(arch='tanh', display_epoch=50, lrate_attractor=0.005, lrate_prediction=0.005, n_attractor_steps=5, n_hidden=10, n_replications=100, noise_level=0.25, seq_len=15, task='reber', train_attr_weights_on_prediction=False, training_epochs=2000)
********************************************************************
mean train accuracy 0.9938
indiv runs  [1.0, 0.995, 0.94999999, 0.95499998, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97000003, 1.0, 1.0, 0.95499998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99000001, 1.0, 0.98500001, 0.98000002, 1.0, 1.0, 1.0, 1.0, 0.98500001, 1.0, 1.0, 0.89999998, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.935, 1.0, 1.0, 1.0, 0.98000002, 1.0, 0.96499997, 1.0, 1.0, 1.0, 1.0, 0.88, 0.995, 1.0, 0.995, 1.0, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.99000001, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.995]
mean epoch 619.58974359
indiv epochs  [751, 501, 201, 401, 101, 1301, 201, 551, 901, 1051, 201, 601, 1051, 1151, 251, 501, 201, 1101, 201, 451, 701, 351, 601, 201, 1101, 1951, 451, 551, 201, 751, 1401, 201, 401, 451, 1551, 151, 351, 301, 501, 1001, 251, 901, 951, 1551, 301, 201, 251, 201, 351, 301, 151, 101, 1001, 351, 901, 501, 701, 951, 251, 1401, 351, 101, 1301, 601, 451, 601, 301, 451, 751, 701, 101, 1601, 601, 1801, 251, 1151, 351, 451]
mean test accuracy 0.876665
indiv runs  [0.84149998, 0.85699999, 0.8175, 0.74400002, 0.8545, 0.93900001, 0.98400003, 0.97600001, 0.85650003, 0.89399999, 0.93849999, 0.88099998, 0.91949999, 0.91949999, 0.89850003, 0.91100001, 0.84149998, 0.79650003, 0.97299999, 0.889, 0.83450001, 0.93699998, 0.8125, 0.84350002, 0.86699998, 0.89099997, 0.93800002, 0.77399999, 0.861, 0.87050003, 0.91750002, 0.87949997, 0.90700001, 0.90649998, 0.86900002, 0.88749999, 0.90200001, 0.92949998, 0.81900001, 0.86500001, 0.94499999, 0.89600003, 0.86000001, 0.9095, 0.76349998, 0.87849998, 0.88550001, 0.79500002, 0.85250002, 0.90350002, 0.88499999, 0.88, 0.76099998, 0.92250001, 0.935, 0.82550001, 0.92500001, 0.87400001, 0.93049997, 0.83099997, 0.91100001, 0.86900002, 0.87900001, 0.89050001, 0.75300002, 0.85750002, 0.91850001, 0.85250002, 0.81150001, 0.87849998, 0.85799998, 0.8405, 0.8495, 0.89850003, 0.92400002, 0.75, 0.87650001, 0.852, 0.84149998, 0.90399998, 0.95950001, 0.87150002, 0.91850001, 0.86849999, 0.88700002, 0.89300001, 0.86000001, 0.90850002, 0.91149998, 0.8495, 0.87099999, 0.81300002, 0.81599998, 0.889, 0.8865, 0.9145, 0.82200003, 0.97549999, 0.92549998, 0.90850002]

If attr net initialized with input_bias, training converges faster but performance is
the same, maybe a tiny bit worse
-------------------------------------------------------------------------------
Kazakov


rnn+attr with single objective
********************************************************************
Namespace(arch='tanh', display_epoch=50, lrate_attractor=0.0, lrate_prediction=0.001, n_attractor_steps=5, n_hidden=20, n_replications=100, noise_level=0.25, report_best_train_performance=True, seq_len=20, task='kazakov', train_attr_weights_on_prediction=True, training_epochs=2000)
********************************************************************
mean train accuracy 0.9459
indiv runs  [0.98500001, 0.99000001, 0.98000002, 0.99250001, 0.92000002, 0.94, 0.95249999, 0.9325, 0.88999999, 0.95499998, 0.86000001, 0.95499998, 0.9375, 0.91000003, 0.92500001, 0.97000003, 0.96749997, 1.0, 0.91000003, 0.97750002, 0.94749999, 0.95499998, 0.95249999, 0.77999997, 0.91250002, 0.93000001, 0.96749997, 0.95249999, 0.75, 0.90750003, 0.93000001, 0.92250001, 0.91000003, 0.95499998, 0.92250001, 0.85750002, 0.98250002, 0.93000001, 0.98250002, 0.92500001, 0.92250001, 0.94499999, 0.97750002, 0.935, 0.97750002, 0.97250003, 0.95249999, 0.995, 0.95749998, 0.97500002, 0.99000001, 0.97250003, 0.95749998, 0.99250001, 0.83999997, 0.96249998, 0.99250001, 0.95249999, 0.8725, 0.98500001, 0.97000003, 0.97000003, 0.92500001, 0.95249999, 0.97750002, 0.97750002, 0.94999999, 0.98750001, 0.98500001, 0.88749999, 0.91750002, 0.92250001, 0.9425, 0.99000001, 0.96499997, 0.98750001, 0.85000002, 0.98000002, 0.9375, 0.96499997, 0.91500002, 0.95499998, 0.97250003, 0.98000002, 0.99250001, 0.9975, 0.96249998, 0.89749998, 0.85500002, 0.99250001, 0.93000001, 0.9375, 0.98500001, 0.97750002, 0.97000003, 0.90750003, 0.99000001, 0.95999998, 0.95499998, 0.96749997]
mean epoch 301.0
indiv epochs  [301]
mean test accuracy 0.767145
indiv runs  [0.81300002, 0.73500001, 0.80650002, 0.86049998, 0.76050001, 0.79650003, 0.80800003, 0.787, 0.68550003, 0.75150001, 0.7475, 0.79500002, 0.7525, 0.79350001, 0.72799999, 0.80199999, 0.71100003, 0.76749998, 0.741, 0.73449999, 0.79900002, 0.76099998, 0.77999997, 0.6875, 0.713, 0.73549998, 0.82349998, 0.773, 0.66399997, 0.73699999, 0.81199998, 0.71450001, 0.73000002, 0.81699997, 0.70700002, 0.82499999, 0.82700002, 0.79299998, 0.77700001, 0.77749997, 0.72399998, 0.78200001, 0.76899999, 0.72649997, 0.80449998, 0.79500002, 0.77999997, 0.75150001, 0.76249999, 0.73400003, 0.7755, 0.79299998, 0.71700001, 0.8355, 0.7245, 0.76200002, 0.81349999, 0.73750001, 0.7665, 0.70249999, 0.79500002, 0.75199997, 0.76749998, 0.86900002, 0.75800002, 0.82950002, 0.80000001, 0.78350002, 0.81150001, 0.74699998, 0.74349999, 0.79100001, 0.80049998, 0.76950002, 0.70300001, 0.71850002, 0.76599997, 0.78399998, 0.778, 0.72149998, 0.77850002, 0.764, 0.829, 0.84249997, 0.72350001, 0.80650002, 0.73150003, 0.71399999, 0.73000002, 0.741, 0.69400001, 0.77200001, 0.74550003, 0.71350002, 0.78100002, 0.76999998, 0.82950002, 0.77149999, 0.8035, 0.7985]


STATE-DENOISING RNN
********************************************************************
Namespace(arch='tanh', display_epoch=50, lrate_attractor=0.0005, lrate_prediction=0.002, n_attractor_steps=5, n_hidden=10, n_replications=100, noise_level=0.25, report_best_train_performance=True, seq_len=20, task='kazakov', train_attr_weights_on_prediction=False, training_epochs=2000)
********************************************************************
mean train accuracy 0.969175
indiv runs  [0.97750002, 0.96749997, 0.9975, 0.9375, 0.98250002, 0.98000002, 0.95749998, 0.90750003, 0.97000003, 0.98500001, 0.97500002, 0.97000003, 0.95249999, 0.97750002, 0.96749997, 0.98250002, 0.97750002, 0.98250002, 0.96499997, 0.95499998, 0.98000002, 0.98500001, 0.97750002, 0.97000003, 0.97500002, 0.97000003, 0.98500001, 0.97250003, 0.97000003, 0.99000001, 0.98250002, 0.97500002, 0.96749997, 0.98500001, 0.98500001, 0.96749997, 0.96499997, 0.95249999, 0.97250003, 0.98500001, 0.9375, 0.98250002, 0.95749998, 0.99000001, 0.96249998, 0.97750002, 0.92250001, 0.96499997, 0.99250001, 0.98500001, 0.99250001, 0.98500001, 0.97250003, 0.95499998, 0.9375, 0.97500002, 0.98250002, 0.97500002, 0.9975, 0.96499997, 0.97500002, 0.97500002, 0.97750002, 0.98000002, 0.97250003, 0.92500001, 0.97500002, 0.95999998, 0.97500002, 0.97500002, 0.9975, 0.98250002, 0.92750001, 0.96749997, 0.98750001, 0.98250002, 0.96499997, 0.99000001, 0.95749998, 0.96499997, 0.98000002, 0.96249998, 0.94749999, 0.97750002, 0.98500001, 0.98750001, 0.97500002, 0.995, 0.98750001, 0.93000001, 0.98500001, 0.9425, 0.89249998, 0.92250001, 0.96499997, 0.95249999, 0.9325, 0.97500002, 0.98250002, 0.9375]
mean epoch nan
indiv epochs  []
mean test accuracy 0.764535
indiv runs  [0.75150001, 0.75599998, 0.90700001, 0.75199997, 0.81050003, 0.76800001, 0.70599997, 0.70850003, 0.70850003, 0.741, 0.85799998, 0.70550001, 0.792, 0.78049999, 0.80199999, 0.75300002, 0.82349998, 0.71149999, 0.81150001, 0.73799998, 0.81, 0.7755, 0.73199999, 0.7385, 0.72049999, 0.72500002, 0.8125, 0.72600001, 0.79449999, 0.75199997, 0.78799999, 0.76450002, 0.7615, 0.82999998, 0.838, 0.70999998, 0.76349998, 0.72250003, 0.72850001, 0.72049999, 0.75150001, 0.89749998, 0.76999998, 0.7385, 0.78899997, 0.7895, 0.74949998, 0.73100001, 0.93550003, 0.74049997, 0.801, 0.74849999, 0.75349998, 0.72600001, 0.73400003, 0.78850001, 0.8545, 0.8075, 0.79149997, 0.82349998, 0.72799999, 0.82200003, 0.71749997, 0.708, 0.7295, 0.75950003, 0.79350001, 0.77999997, 0.75150001, 0.72850001, 0.85000002, 0.83149999, 0.71700001, 0.80199999, 0.77749997, 0.78149998, 0.69950002, 0.77200001, 0.727, 0.74199998, 0.78100002, 0.74150002, 0.81449997, 0.74449998, 0.704, 0.76200002, 0.72899997, 0.88450003, 0.7335, 0.71149999, 0.74349999, 0.69300002, 0.72600001, 0.671, 0.796, 0.74299997, 0.759, 0.73250002, 0.73500001, 0.71149999]


BASIC RNN
********************************************************************
Namespace(arch='tanh', display_epoch=50, lrate_attractor=0.002, lrate_prediction=0.002, n_attractor_steps=0, n_hidden=10, n_replications=100, noise_level=0.25, report_best_train_performance=True, seq_len=20, task='kazakov', train_attr_weights_on_prediction=False, training_epochs=2000)
********************************************************************
mean train accuracy 0.977625
indiv runs  [0.9975, 0.97500002, 0.97250003, 0.98000002, 0.97250003, 0.97500002, 0.98500001, 0.96249998, 0.98250002, 0.97250003, 0.98750001, 0.96249998, 0.94999999, 0.98250002, 0.98250002, 0.95749998, 0.98500001, 0.97500002, 0.96249998, 0.98250002, 0.99250001, 0.98500001, 0.98000002, 0.97500002, 0.96749997, 0.99250001, 0.995, 0.98000002, 0.98250002, 0.97250003, 0.9975, 0.98000002, 0.97250003, 0.98500001, 0.995, 0.96499997, 0.98000002, 0.97750002, 0.96249998, 0.97000003, 0.96499997, 0.99000001, 0.97000003, 0.98000002, 0.99000001, 0.99000001, 0.99000001, 0.98750001, 0.99250001, 0.96499997, 0.98500001, 0.98000002, 0.97500002, 0.98250002, 0.9425, 0.98000002, 0.98750001, 0.97750002, 0.99000001, 0.99000001, 0.98500001, 0.98250002, 0.97500002, 0.97000003, 0.98500001, 0.97000003, 0.99000001, 0.97000003, 0.97750002, 0.97500002, 0.98250002, 0.98750001, 0.96749997, 0.98000002, 0.97500002, 0.98250002, 0.98000002, 0.98000002, 0.97000003, 0.96499997, 0.97500002, 0.98250002, 0.995, 0.98250002, 0.95249999, 0.98750001, 0.98500001, 0.96499997, 0.97750002, 0.96749997, 0.98500001, 0.95499998, 0.95499998, 0.96499997, 0.97250003, 0.98000002, 0.98000002, 0.99250001, 0.96749997, 0.97000003]
mean epoch nan
indiv epochs  []
mean test accuracy 0.76751
indiv runs  [0.78100002, 0.75599998, 0.82099998, 0.81400001, 0.7105, 0.722, 0.74650002, 0.71799999, 0.7295, 0.74550003, 0.84249997, 0.7335, 0.80650002, 0.77249998, 0.81599998, 0.72500002, 0.73799998, 0.73949999, 0.7985, 0.70999998, 0.88550001, 0.78500003, 0.74650002, 0.7295, 0.755, 0.73299998, 0.89349997, 0.75599998, 0.76749998, 0.72649997, 0.90549999, 0.77149999, 0.7755, 0.80500001, 0.81300002, 0.71200001, 0.80250001, 0.73799998, 0.69199997, 0.70450002, 0.71200001, 0.89349997, 0.77600002, 0.7525, 0.78600001, 0.787, 0.77700001, 0.7385, 0.89850003, 0.73250002, 0.81, 0.79049999, 0.75800002, 0.74900001, 0.68650001, 0.83149999, 0.84450001, 0.80650002, 0.78549999, 0.87300003, 0.73699999, 0.76249999, 0.7385, 0.73299998, 0.73699999, 0.77350003, 0.82050002, 0.78100002, 0.75300002, 0.76499999, 0.75550002, 0.81449997, 0.7665, 0.78850001, 0.75199997, 0.77850002, 0.71249998, 0.77899998, 0.70749998, 0.74949998, 0.764, 0.75300002, 0.81349999, 0.74550003, 0.65499997, 0.77450001, 0.7705, 0.82599998, 0.76349998, 0.71249998, 0.7385, 0.73299998, 0.764, 0.71799999, 0.78899997, 0.7525, 0.75099999, 0.74449998, 0.71700001, 0.74849999]

-------------------------------------------------------------------------------
Wed Dec 13 12:42:39 MST 2017

parity replication

********************************************************************
Namespace(arch='tanh', display_epoch=100, lrate_attractor=0.008, lrate_prediction=0.008, n_attractor_steps=0, n_hidden=5, n_replications=100, noise_level=0.25, report_best_train_performance=False, seq_len=5, task='parity', train_attr_weights_on_prediction=False, training_epochs=4000)
********************************************************************
mean train accuracy 0.933438
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.9375, 1.0, 0.6875, 1.0, 1.0, 0.90625, 0.90625, 1.0, 0.96875, 1.0, 0.53125, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 0.75, 0.75, 0.96875, 0.6875, 1.0, 0.875, 0.75, 0.96875, 0.84375, 0.90625, 1.0, 0.53125, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.5625, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.5, 0.96875, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.96875, 0.90625, 1.0, 1.0, 1.0, 0.78125, 1.0, 1.0, 0.9375, 0.96875, 0.96875, 0.53125, 1.0, 1.0, 0.53125, 0.8125, 1.0, 0.96875, 0.90625, 1.0, 1.0]

attractor net + lower-diagonal weight initialization
********************************************************************
Namespace(arch='tanh', display_epoch=100, lrate_attractor=0.008, lrate_prediction=0.008, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, report_best_train_performance=False, seq_len=5, task='parity', train_attr_weights_on_prediction=False, training_epochs=4000)
********************************************************************
mean train accuracy 0.957187
indiv runs  [1.0, 0.96875, 0.84375, 1.0, 1.0, 0.96875, 1.0, 1.0, 0.90625, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.90625, 0.96875, 0.53125, 1.0, 1.0, 0.71875, 0.96875, 0.75, 0.9375, 1.0, 1.0, 1.0, 0.96875, 0.96875, 0.90625, 0.96875, 0.90625, 1.0, 0.9375, 1.0, 0.9375, 0.96875, 0.71875, 0.96875, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 0.96875, 0.96875, 1.0, 0.96875, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 0.78125, 0.96875, 0.53125, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0]


attractor net + original constrained initialization
********************************************************************
Namespace(arch='tanh', display_epoch=100, lrate_attractor=0.008, lrate_prediction=0.008, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, report_best_train_performance=False, seq_len=5, task='parity', train_attr_weights_on_prediction=False, training_epochs=4000)
********************************************************************
mean train accuracy 0.948438
indiv runs  [0.96875, 1.0, 0.96875, 1.0, 0.96875, 0.84375, 0.65625, 0.96875, 1.0, 0.71875, 1.0, 0.96875, 0.9375, 1.0, 1.0, 0.90625, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.625, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.96875, 0.90625, 1.0, 0.9375, 0.96875, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 0.96875, 0.9375, 1.0, 0.90625, 0.96875, 1.0, 1.0, 0.96875, 0.8125, 0.875, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.96875, 0.59375, 0.9375, 0.875, 0.96875, 1.0, 0.78125, 1.0, 0.96875, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.875, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.65625, 1.0, 1.0, 1.0, 0.96875, 0.9375, 0.96875, 0.625, 1.0, 1.0, 1.0]


attractor net + no constraints on attractor weights
********************************************************************
Namespace(arch='tanh', display_epoch=100, lrate_attractor=0.008, lrate_prediction=0.008, n_attractor_steps=5, n_hidden=5, n_replications=100, noise_level=0.25, report_best_train_performance=False, seq_len=5, task='parity', train_attr_weights_on_prediction=False, training_epochs=4000)
********************************************************************
mean train accuracy 0.9575
indiv runs  [0.96875, 0.8125, 1.0, 1.0, 0.84375, 1.0, 1.0, 0.875, 1.0, 0.96875, 1.0, 1.0, 0.90625, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 0.96875, 1.0, 0.84375, 1.0, 0.625, 0.96875, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 1.0, 0.90625, 1.0, 1.0, 0.96875, 0.84375, 0.90625, 0.96875, 0.875, 0.96875, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.96875, 1.0, 0.75, 0.8125, 0.96875, 1.0, 0.96875, 0.5, 0.96875, 0.96875, 0.90625, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.84375, 0.65625, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 1.0, 1.0, 0.84375, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0]

