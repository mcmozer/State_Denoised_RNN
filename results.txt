-------------------------------------------------------------------------------
BASELINE RESULTS WITHOUT ATTRACTOR NET
-------------------------------------------------------------------------------
arch vanilla seq_len 5 training_epochs 10000

mean accuracy 0.930313
indiv runs  [1.0, 1.0, 0.96875, 1.0, 1.0, 0.53125, 0.90625, 1.0, 1.0, 0.96875, 1.0, 0.96875, 0.90625, 0.96875, 0.96875, 1.0, 0.96875, 0.90625, 0.9375, 1.0, 0.96875, 1.0, 0.71875, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 0.90625, 1.0, 0.53125, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.71875, 1.0, 0.65625, 1.0, 0.96875, 0.53125, 0.875, 0.96875, 0.96875, 1.0, 1.0, 0.75, 1.0, 1.0, 1.0, 0.53125, 1.0, 0.96875, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.71875, 0.96875, 0.96875, 0.96875, 1.0, 0.90625, 1.0, 0.8125, 1.0, 1.0, 0.71875, 0.71875, 1.0, 1.0, 1.0, 1.0, 0.53125, 0.53125, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 2972.92982456
indiv epochs  [2601, 8801, 3801, 1201, 2001, 2201, 9601, 7201, 1001, 6201, 801, 1401, 1001, 6201, 1401, 1001, 2001, 4401, 1001, 3401, 6201, 601, 601, 801, 5201, 1801, 1001, 5401, 7001, 1001, 4801, 4601, 2001, 2601, 1601, 1001, 3201, 8001, 801, 1001, 2201, 1401, 1001, 1201, 3201, 3801, 4001, 5201, 3601, 3801, 1801, 401, 1801, 1201, 5001, 1001, 2401]


arch LSTM seq_len 5 training_epochs 10000

mean accuracy 0.951875
indiv runs  [1.0, 0.5625, 1.0, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.59375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.4375, 0.5625, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 0.5625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.46875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 2907.66666667
indiv epochs  [2201, 4601, 1801, 3601, 1001, 2601, 3801, 1201, 1001, 3601, 1601, 3801, 1001, 6001, 9601, 1401, 1401, 1001, 5201, 601, 3001, 601, 1801, 2601, 3001, 801, 2401, 1201, 4001, 1401, 1401, 601, 4201, 3001, 1001, 1401, 8801, 1201, 2201, 7401, 3201, 10001, 1801, 601, 8401, 1201, 601, 4201, 1401, 2201, 2601, 2601, 2401, 2201, 1401, 3201, 1801, 5001, 601, 2801, 6801, 1401, 3001, 9401, 5201, 1401, 1001, 6201, 3201, 1601, 1001, 2001, 2801, 2001, 3801, 4201, 3801, 2001, 1401, 3601, 3001, 601, 3801, 3201, 2201, 2601, 1601, 3801, 5601, 2201]


arch GRU seq_len 5 training_epochs 10000

mean accuracy 1.0
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 919.0
indiv epochs  [1201, 1001, 1001, 1401, 401, 801, 1001, 2001, 601, 601, 401, 401, 401, 601, 601, 401, 1201, 401, 601, 401, 1401, 801, 1201, 1201, 801, 401, 2201, 2401, 401, 601, 1001, 401, 401, 601, 601, 1801, 1201, 3601, 1601, 401, 601, 801, 1001, 1801, 601, 401, 801, 401, 401, 201, 601, 401, 401, 401, 2001, 801, 2401, 401, 3801, 2001, 801, 401, 801, 1001, 801, 801, 1001, 1001, 1001, 1601, 401, 1601, 1601, 1001, 1001, 1401, 401, 401, 601, 601, 401, 601, 1401, 2001, 801, 1001, 401, 601, 1201, 1201, 401, 401, 601, 801, 801, 401, 401, 801, 401, 401]


-------------------------------------------------------------------------------
state-denoising RNNs without the secondary training objective

- increasing from 0 to 3 # of attractor steps does not help
-------------------------------------------------------------------------------
arch GRU SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 0
mean accuracy 0.998438
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1805.12371134
indiv epochs  [2001, 9601, 1401, 2801, 1001, 1601, 1801, 5601, 1401, 1801, 1001, 801, 1201, 1401, 1001, 1201, 2201, 801, 1801, 601, 1801, 1601, 1601, 1601, 801, 1801, 4201, 601, 1001, 1201, 601, 1001, 801, 2001, 1601, 1801, 1401, 3001, 801, 1401, 3201, 2801, 2001, 1001, 1601, 401, 801, 601, 1201, 801, 1401, 601, 1801, 1201, 2601, 601, 2401, 2401, 801, 1001, 1801, 1801, 1401, 1601, 1201, 2001, 7201, 1001, 8601, 7601, 1601, 3601, 2601, 1201, 1001, 1001, 1401, 1201, 1801, 1801, 3601, 1401, 1601, 801, 1201, 1801, 2001, 601, 1201, 1401, 1201, 1401, 601, 1001, 1401, 801, 801]

arch GRU SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3
mean accuracy 0.958438
indiv runs  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1906.61797753
indiv epochs  [1601, 1201, 1201, 1801, 2801, 5801, 1001, 3201, 1201, 1401, 601, 801, 1801, 801, 801, 4801, 401, 1801, 2401, 5401, 801, 2001, 1201, 7601, 401, 1601, 1001, 1201, 2401, 401, 1401, 1601, 1201, 1601, 2201, 3601, 601, 1401, 1801, 1201, 3401, 3201, 801, 401, 601, 601, 801, 601, 7601, 1801, 3801, 401, 3801, 3601, 1401, 6801, 2201, 1801, 1001, 1001, 1201, 2001, 1201, 1601, 801, 1601, 801, 601, 4401, 2001, 1001, 3401, 1001, 2001, 1201, 801, 2601, 1601, 1801, 401, 601, 801, 801, 5001, 401, 3601, 3401, 801, 601]


arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 0
mean accuracy 0.9025
indiv runs  [0.9375, 0.75, 0.75, 1.0, 0.96875, 0.65625, 1.0, 0.96875, 0.96875, 0.71875, 0.96875, 0.96875, 0.9375, 1.0, 0.90625, 1.0, 0.96875, 0.75, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 0.78125, 0.96875, 0.9375, 0.96875, 1.0, 1.0, 0.53125, 0.9375, 0.625, 1.0, 1.0, 0.96875, 0.8125, 1.0, 0.71875, 1.0, 0.96875, 0.65625, 0.53125, 0.96875, 1.0, 1.0, 1.0, 0.75, 0.53125, 1.0, 1.0, 0.53125, 1.0, 0.78125, 0.875, 1.0, 1.0, 1.0, 0.75, 1.0, 0.75, 1.0, 0.75, 0.53125, 0.96875, 0.9375, 1.0, 1.0, 0.96875, 0.96875, 1.0, 0.96875, 0.8125, 0.96875, 0.75, 0.90625, 1.0, 0.9375, 1.0, 0.96875, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.5625, 1.0, 0.8125, 0.96875, 1.0, 0.90625, 1.0, 0.96875, 1.0, 0.9375, 1.0, 0.96875]
mean epoch 2986.36585366
indiv epochs  [4601, 2201, 1601, 3801, 2801, 5801, 2401, 3001, 1001, 2001, 2601, 8201, 601, 4001, 6401, 1201, 4001, 5001, 1001, 2001, 1201, 2801, 8801, 2801, 2201, 1201, 3201, 3801, 2601, 801, 1001, 5401, 5601, 2201, 1801, 1401, 3201, 1401, 601, 4001, 2201]

arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3
mean accuracy 0.914062
indiv runs  [0.9375, 1.0, 1.0, 0.75, 1.0, 0.59375, 1.0, 0.8125, 0.9375, 0.75, 0.96875, 0.90625, 0.96875, 1.0, 0.75, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.90625, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.8125, 0.71875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 0.96875, 0.65625, 1.0, 1.0, 0.96875, 0.8125, 0.9375, 0.84375, 1.0, 0.96875, 0.53125, 0.75, 0.96875, 0.84375, 1.0, 1.0, 0.75, 1.0, 1.0, 0.8125, 0.53125, 1.0, 0.8125, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.65625, 0.75, 0.6875, 0.96875, 0.9375, 1.0, 1.0, 0.90625, 0.96875, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 0.96875, 1.0, 0.8125, 0.90625, 1.0, 1.0, 1.0, 1.0, 0.75, 0.8125, 0.96875, 0.96875, 0.84375, 1.0, 0.96875, 1.0, 0.96875, 0.96875, 0.96875]
mean epoch 2935.7826087
indiv epochs  [9401, 2401, 2201, 1401, 3801, 3401, 1601, 9401, 2201, 2001, 801, 2601, 3001, 2401, 4401, 1201, 2601, 401, 1001, 5401, 2001, 1001, 601, 8601, 2801, 801, 8601, 1001, 801, 1801, 3001, 1601, 801, 4801, 5201, 9201, 1001, 1401, 1201, 2401, 1001, 7001, 801, 1201, 2601, 2201]


-------------------------------------------------------------------------------
Sat Nov 25 16:14:02 PST 2017
Two noise levels of attractor net training with 5 attractor steps -- beats out 
vanilla in both accuracy and # training epochs for success

arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.947187
indiv runs  [1.0, 1.0, 0.9375, 0.53125, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.875, 0.9375, 0.53125, 0.96875, 1.0, 1.0, 0.84375, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.53125, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 0.96875, 1.0, 0.90625, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.71875, 0.96875, 0.96875, 1.0, 1.0, 0.96875]
mean epoch 1289.23529412
indiv epochs  [401, 601, 401, 201, 401, 601, 601, 2601, 801, 201, 401, 1001, 1601, 6401, 201, 201, 1201, 201, 401, 401, 401, 401, 3201, 801, 201, 3001, 2201, 201, 3401, 2401, 601, 1401, 201, 2601, 601, 401, 401, 2001, 2401, 401, 401, 601, 3801, 801, 801, 601, 1601, 1001, 201, 1601, 201, 201, 601, 401, 201, 201, 401, 1401, 7201, 2001, 2401, 401, 8801, 601, 601, 201, 601, 3801]

arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.125 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.945625
indiv runs  [1.0, 0.96875, 1.0, 0.53125, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.53125, 0.9375, 0.53125, 0.96875, 1.0, 1.0, 0.84375, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.96875, 0.53125, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.90625, 0.9375, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 0.96875, 0.90625, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 0.9375, 0.96875, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.6875, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1578.77777778
indiv epochs  [801, 1401, 401, 201, 801, 801, 601, 1201, 601, 801, 401, 1401, 1401, 2601, 401, 201, 9201, 4201, 2801, 1201, 801, 1001, 3001, 801, 7201, 401, 4201, 601, 201, 1001, 601, 1601, 2401, 201, 1201, 401, 201, 801, 1601, 3001, 401, 3201, 401, 5401, 2801, 401, 601, 1401, 401, 201, 201, 601, 201, 401, 401, 201, 201, 401, 1201, 9201, 4201, 401, 1601, 2201, 601, 601, 401, 401, 9801, 1001, 801, 801]

update frequency 5
arch tanh SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 5
lrate prediction LRATE_PREDICTION  lrate attractor  0.008
mean accuracy 0.94875
indiv runs  [1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.53125, 0.625, 0.53125, 0.96875, 1.0, 1.0, 0.8125, 1.0, 1.0, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.90625, 0.53125, 1.0, 0.65625, 1.0, 1.0, 1.0, 1.0, 0.9375, 0.96875, 0.84375, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 0.96875, 0.96875, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.8125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.65625, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.53125, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 0.90625, 1.0, 0.90625, 1.0, 1.0, 1.0, 1.0, 1.0]
mean epoch 1449.64864865
indiv epochs  [401, 801, 3001, 401, 201, 801, 801, 601, 801, 601, 201, 401, 801, 801, 9801, 201, 201, 401, 1201, 401, 2401, 401, 801, 3201, 401, 1801, 601, 201, 801, 2201, 401, 1801, 201, 801, 401, 201, 5201, 2001, 4001, 401, 3801, 401, 2601, 1001, 1801, 601, 401, 1601, 1801, 401, 201, 2401, 2001, 201, 2201, 2001, 201, 801, 401, 401, 401, 401, 1601, 10001, 801, 801, 1601, 601, 3801, 401, 8801, 601, 601, 601]



-------------------------------------------------------------------------------
integrated objective function: denoising + prediction
varying lambda

arch vanilla SEQ_LEN 5 training_epochs 10000 noise_level 0.25 n_attr_steps 3 
lambda 0.08 mean accuracy 0.91375
lambda 0.04 mean accuracy 0.913125
lambda 0.01 mean accuracy 0.91
lambda 0.02 mean accuracy 0.91375
lambda 0.0  mean accuracy 0.92375
-------------------------------------------------------------------------------

testing attractor net (attractor_test.py)
10 replications, mean test relative distance 

unconstrained + h_out=W*tanh(h_net)+b	0.288803
zero diagonal + h_out=W*tanh(h_net)+b	0.311553
symmetry + h_out=W*tanh(h_net)+b	0.392045
0diag+symm + h_out=W*tanh(h_net)+b	0.430753
unconstrained + h_out=tanh(W*h_net+b)	0.490302
0diag+symm + h_out=tanh(W*h_net+b) 	0.577489
0diag+symm + h_out=tanh(h_net)      	0.979404
  

final xform = linear transform of [-1,+1] scaled output
-------------------------------------------------------------------------------

MIKE: try different variants of attractor net
- lose symmetry and diag zeros
- add linear transform after net
- add bias to internal dynamics
MIKE: allow attractor weights to be trained by prediction objective?
MIKE: specify schedule of updating:  P, A, or B-both
   e.g., PPPPPPPPPPAAAAAAAAAA [and then repeat]


   `
